# Hybrid Recommendation Experiment: GRPO/DPO Ranking + Bandit Exploration

# ---
# Cell 1: åœºæ™¯è®¾å®šä¸ç»“æ„è¯´æ˜ï¼ˆMarkdownï¼‰
"""
# æ¨èç³»ç»Ÿä¸­çš„æ··åˆæ’åºä¸ Bandit/RL æ¢ç´¢å®éªŒ

## ğŸ¯ å®éªŒèƒŒæ™¯

æœ¬å®éªŒæ¨¡æ‹Ÿäº†ä¸€ä¸ªå·¥ä¸šæ¨èç³»ç»Ÿçš„å…¸å‹æ¶æ„ï¼Œç»“åˆä¸»æµçš„ pairwise rankingï¼ˆå¦‚ GRPO/DPO æŸå¤±ï¼‰å’Œæ¢ç´¢å‹ bandit ç­–ç•¥ï¼Œç”¨äºå…¼é¡¾å¤§è§„æ¨¡æ’åºå’Œå†·å¯åŠ¨/é•¿æœŸæ”¶ç›Šã€‚

- ä¸»ä½“æ¨¡å‹ï¼šGRPO/DPO åŒå¡”ç»“æ„ï¼Œpairwise preference æ’åºè®­ç»ƒ
- æ¢ç´¢ç­–ç•¥ï¼šä¸ºæ–°ç”¨æˆ·/æ–°ç‰©å“å¼•å…¥ epsilon-greedy bandit æ¨è
- Session æ¨¡æ‹Ÿï¼šç”¨æˆ·å¤šè½®äº¤äº’ï¼Œç´¯è®¡é•¿æœŸ rewardï¼Œæ”¯æŒ RL-style è¯„ä¼°

---

## ğŸ—ï¸ æ¨¡å‹ä¸ç­–ç•¥ç»“æ„

1. **ä¸»å¬å›æ’åºæ¨¡å—**ï¼šç”¨ pairwise loss ä¼˜åŒ– embeddingï¼ˆåŒå¡”æ¨¡å‹ï¼‰
2. **Bandit æ¢ç´¢æ¨¡å—**ï¼šå¯¹å†·å¯åŠ¨ user/item ç”¨ epsilon-greedy ç­–ç•¥æ¨èï¼ˆæ¢ç´¢ vs åˆ©ç”¨ï¼‰
3. **Session å¤šè½®ä»¿çœŸ**ï¼šæ¯è½®æ¨èåé‡‡é›†åé¦ˆï¼Œç´¯è®¡ rewardï¼Œåˆ†æä¸» ranking ä¸æ¢ç´¢ bandit è¡¨ç°å·®å¼‚

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡
- Pairwise æ’åºå‡†ç¡®ç‡
- å„ç”¨æˆ·ç»„ç´¯è®¡ reward
- bandit ç­–ç•¥åˆ†æµä¸‹æ¢ç´¢ä¸ exploitation æ¯”ä¾‹
- session CTR/æ´»è·ƒç­‰é•¿æœŸæŒ‡æ ‡
"""

# ---
# Cell 2: æ•°æ®æ¨¡æ‹Ÿä¸å¤šè½® session æ„é€ 
import numpy as np
import pandas as pd
np.random.seed(42)

n_users, n_items = 200, 300
user_ids = [f'user_{i}' for i in range(n_users)]
item_ids = [f'item_{j}' for j in range(n_items)]

# æ ‡è®°éƒ¨åˆ†ç”¨æˆ·/ç‰©å“ä¸ºå†·å¯åŠ¨
cold_start_users = set(np.random.choice(user_ids, size=20, replace=False))
cold_start_items = set(np.random.choice(item_ids, size=30, replace=False))

# ç”Ÿæˆç”¨æˆ·è¡Œä¸ºæ—¥å¿—ï¼ˆå¤š sessionï¼Œæ¯ session 3~6æ¬¡æ¨èï¼‰
sessions = []
for u in user_ids:
    n_sessions = np.random.randint(2, 6)
    for s in range(n_sessions):
        session = []
        for t in range(np.random.randint(3, 7)):
            i = np.random.choice(item_ids)
            click = np.random.randint(0, 4)  # ç‚¹å‡»æ¬¡æ•°
            buy = np.random.choice([0, 1], p=[0.94, 0.06])
            stay = np.random.exponential(2)
            dislike = np.random.choice([0, 1], p=[0.97, 0.03])
            session.append({'user': u, 'item': i, 'click': click, 'buy': buy, 'stay': stay, 'dislike': dislike})
        sessions.append(session)

# æ„é€  pairwise åå¥½ä¸‰å…ƒç»„ï¼ˆæ­£æ ·æœ¬ï¼šå¤šæ¬¡ç‚¹å‡»/è´­ä¹°/é•¿åœç•™ï¼Œè´Ÿæ ·æœ¬ï¼šç‚¹ğŸ‘ï¼‰
tuple_list = []
for session in sessions:
    u = session[0]['user']
    pos = [x['item'] for x in session if ((x['click'] >= 2) or (x['buy'] == 1) or (x['stay'] > 3)) and x['dislike']==0]
    neg = [x['item'] for x in session if x['dislike']==1]
    if len(pos) > 0 and len(neg) > 0:
        n = min(len(pos), len(neg))
        tuple_list += [(u, pos[i % len(pos)], neg[i % len(neg)]) for i in range(n)]

print(f"pairwiseä¸‰å…ƒç»„æ ·æœ¬æ•°: {len(tuple_list)}")

# ---
# Cell 3: ç‰¹å¾æ¨¡æ‹Ÿ
embedding_dim = 32
user_feat = {u: np.random.normal(size=(embedding_dim,)).astype(np.float32) for u in user_ids}
item_feat = {i: np.random.normal(size=(embedding_dim,)).astype(np.float32) for i in item_ids}

# ---
# Cell 4: åŒå¡”æ¨¡å‹ç»“æ„ + Ranking è®­ç»ƒï¼ˆPyTorchï¼‰
import torch
import torch.nn as nn
import torch.nn.functional as F

device = 'cuda' if torch.cuda.is_available() else 'cpu'

class Tower(nn.Module):
    def __init__(self, in_dim, out_dim=32):
        super().__init__()
        self.fc = nn.Linear(in_dim, out_dim)
        self.act = nn.ReLU()
    def forward(self, x):
        return self.act(self.fc(x))

class GRPOModel(nn.Module):
    def __init__(self, embed_dim=32):
        super().__init__()
        self.user_tower = Tower(embed_dim, embed_dim)
        self.item_tower = Tower(embed_dim, embed_dim)
    def forward(self, u, i):
        u_emb = self.user_tower(u)
        i_emb = self.item_tower(i)
        return (u_emb * i_emb).sum(-1)

model = GRPOModel(embedding_dim).to(device)

# ---
# Cell 5: Pairwise Dataset & DataLoader
from torch.utils.data import Dataset, DataLoader

class PairwiseDataset(Dataset):
    def __init__(self, triplets):
        self.triplets = triplets
    def __len__(self):
        return len(self.triplets)
    def __getitem__(self, idx):
        u, ip, ineg = self.triplets[idx]
        return (
            torch.tensor(user_feat[u]),
            torch.tensor(item_feat[ip]),
            torch.tensor(item_feat[ineg])
        )

dataset = PairwiseDataset(tuple_list)
dataloader = DataLoader(dataset, batch_size=256, shuffle=True)

# ---
# Cell 6: GRPO/DPO æŸå¤±å‡½æ•°

def grpo_loss(pos_score, neg_score, beta=0.1):
    kl = beta * (pos_score.pow(2).mean() + neg_score.pow(2).mean())
    pref = -torch.log(torch.sigmoid(pos_score - neg_score)).mean()
    return pref + kl

def dpo_loss(pos_score, neg_score, beta=0.1):
    margin = (pos_score - neg_score) / beta
    return -torch.log(torch.sigmoid(margin)).mean()

# ---
# Cell 7: Ranking è®­ç»ƒä¸»å¾ªç¯
from tqdm import tqdm
import matplotlib.pyplot as plt

epochs = 10
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_curve = []
acc_curve = []

for epoch in range(epochs):
    model.train()
    total_loss, correct, total = 0, 0, 0
    for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}'):
        u, ip, ineg = [x.float().to(device) for x in batch]
        pos_score = model(u, ip)
        neg_score = model(u, ineg)
        #loss = grpo_loss(pos_score, neg_score, beta=0.1)
        loss = dpo_loss(pos_score, neg_score, beta=0.1)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * u.shape[0]
        correct += (pos_score > neg_score).sum().item()
        total += u.shape[0]
    epoch_loss = total_loss / total
    epoch_acc = correct / total
    loss_curve.append(epoch_loss)
    acc_curve.append(epoch_acc)
    print(f"Epoch {epoch+1}: loss={epoch_loss:.4f}, pairwise acc={epoch_acc:.4f}")

# ---
# Cell 8: Bandit (epsilon-greedy) æ¢ç´¢ç­–ç•¥å®ç°ä¸è¯„ä¼°

# ç®€å•æ¨¡æ‹Ÿï¼šæ¯ä¸ª session å†…
# å¯¹ä¸» ranking ç”¨æˆ·ç”¨æ’åºæœ€é«˜å¾—åˆ†æ¨èï¼Œå¯¹ cold_start ç”¨æˆ· epsilon æ¦‚ç‡éšæœºæ¢ç´¢
np.random.seed(43)
def recommend_bandit(user, user_emb, candidate_items, model, eps=0.2):
    if user in cold_start_users:
        if np.random.rand() < eps:
            # æ¢ç´¢ï¼ˆéšæœºæ¨èï¼‰
            return np.random.choice(candidate_items)
        # åˆ©ç”¨ï¼ˆç”¨æ¨¡å‹æ¨èï¼‰
    # æ’åºåˆ©ç”¨ï¼ˆç›´æ¥ç”¨æ¨¡å‹è¾“å‡ºåˆ†æ•°ï¼‰
    user_vec = torch.tensor(user_emb[user]).float().unsqueeze(0).to(device)
    item_vecs = torch.stack([torch.tensor(item_feat[i]).float() for i in candidate_items]).to(device)
    scores = model.user_tower(user_vec) @ model.item_tower(item_vecs).T
    top_idx = torch.argmax(scores, dim=1).item()
    return candidate_items[top_idx]

# ç»Ÿè®¡æ¢ç´¢åˆ†æµã€ç´¯è®¡ reward
bandit_stats = {"explore":0, "exploit":0, "reward":[], "cold":[], "warm":[]}
session_reward_curve = []

for session in sessions[:1500]: # åªæ¨¡æ‹Ÿéƒ¨åˆ†sessionèŠ‚çœæ—¶é—´
    u = session[0]['user']
    candidate_items = [x['item'] for x in session]
    ground_truth = {(x['item'], x['buy'] or (x['click']>=2) or (x['stay']>3)) for x in session}
    recommended = recommend_bandit(u, user_feat, candidate_items, model, eps=0.2)
    # ç®€åŒ– reward: è‹¥æ¨èå‘½ä¸­æ­£æ ·æœ¬ reward=1ï¼Œå¦åˆ™0
    reward = 1 if (recommended, True) in ground_truth else 0
    if u in cold_start_users and np.random.rand() < 0.2:
        bandit_stats["explore"] += 1
    else:
        bandit_stats["exploit"] += 1
    bandit_stats["reward"].append(reward)
    (bandit_stats["cold"] if u in cold_start_users else bandit_stats["warm"]).append(reward)
    session_reward_curve.append(np.mean(bandit_stats["reward"]))

print(f"æ¢ç´¢åˆ†æµæ¬¡æ•°: {bandit_stats['explore']}, åˆ©ç”¨æ¬¡æ•°: {bandit_stats['exploit']}")
print(f"æ€»ä½“å¹³å‡ reward: {np.mean(bandit_stats['reward']):.3f}")
print(f"å†·å¯åŠ¨ç”¨æˆ· reward: {np.mean(bandit_stats['cold']):.3f}, ä¸»æµç”¨æˆ· reward: {np.mean(bandit_stats['warm']):.3f}")

# ---
# Cell 9: Loss & Reward æ›²çº¿å¯è§†åŒ–
plt.figure(figsize=(14,4))
plt.subplot(1,3,1)
plt.plot(loss_curve, label='Ranking Loss')
plt.xlabel('Epoch'); plt.legend(); plt.title('Ranking Loss')
plt.subplot(1,3,2)
plt.plot(acc_curve, label='Pairwise Accuracy')
plt.xlabel('Epoch'); plt.legend(); plt.title('Pairwise Accuracy')
plt.subplot(1,3,3)
plt.plot(session_reward_curve, label='Bandit Mean Reward')
plt.xlabel('Session'); plt.legend(); plt.title('Bandit Mean Reward')
plt.tight_layout()
plt.show()

# ---
# Cell 10: æ€»ç»“ä¸å»ºè®®ï¼ˆMarkdownï¼‰
"""
## âœ… å®éªŒæ€»ç»“
- æœ¬å®éªŒå®ç°äº†å·¥ä¸šæ¨èç³»ç»Ÿä¸­ä¸»æµ ranking+bandit æ¢ç´¢æ··åˆæ¶æ„ï¼Œæ—¢ä¿è¯å¤§è§„æ¨¡æ’åºèƒ½åŠ›ï¼Œä¹Ÿæ”¯æŒå†·å¯åŠ¨/æ¢ç´¢ã€‚
- ç»“æœå±•ç¤ºäº† pairwise ranking å­¦ä¹ æ›²çº¿ï¼Œbandit åˆ†æµã€å†·å¯åŠ¨å’Œä¸»æµç”¨æˆ· reward å¯¹æ¯”ç­‰ã€‚

### ğŸ“ˆ åç»­å¯æ‰©å±•ï¼š
- æ”¯æŒæ›´å¤æ‚çš„ RL ç­–ç•¥ï¼ˆå¦‚ Thompson Sampling, policy gradientï¼‰
- ç”¨æˆ·è¡Œä¸ºçœŸå®åºåˆ—å»ºæ¨¡ï¼Œé•¿æœŸ session reward/ç”¨æˆ·ç•™å­˜
- ä¸çœŸå®å·¥ä¸šæ—¥å¿—/çº¿ä¸ŠABå®éªŒå¯¹æ¥è¯„ä¼°
"""
