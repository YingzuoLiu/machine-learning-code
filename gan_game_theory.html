<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GAN博弈论推导技术报告</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['\\(', '\\)'], ['$', '$']],
                displayMath: [['\\[', '\\]'], ['$$', '$$']],
                tags: 'ams'
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <style>
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.7;
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            color: #2c3e50;
        }
        
        .container {
            background: white;
            padding: 50px;
            border-radius: 15px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 3px solid #3498db;
            padding-bottom: 20px;
            margin-bottom: 30px;
            font-size: 2.2em;
        }
        
        h2 {
            color: #34495e;
            border-left: 5px solid #3498db;
            padding-left: 20px;
            margin-top: 40px;
            margin-bottom: 20px;
            background: linear-gradient(90deg, #ecf0f1 0%, transparent 100%);
            padding: 15px 0 15px 20px;
        }
        
        h3 {
            color: #2980b9;
            margin-top: 30px;
        }
        
        .goal-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
            font-weight: 500;
        }
        
        .formula-box {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            border-left: 5px solid #ff6b6b;
            padding: 20px;
            border-radius: 0 10px 10px 0;
            margin: 20px 0;
        }
        
        .result-box {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            border: 3px solid #26d0ce;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            margin: 25px 0;
            font-size: 1.1em;
            font-weight: 600;
        }
        
        ul, ol {
            margin: 20px 0;
            padding-left: 25px;
        }
        
        li {
            margin: 8px 0;
        }
        
        .note {
            background: #e8f4f8;
            border-left: 4px solid #2196F3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 5px 5px 0;
            font-style: italic;
        }
        
        .example-box {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        
        code {
            background: #f1f2f6;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        hr {
            border: none;
            height: 2px;
            background: linear-gradient(90deg, transparent, #3498db, transparent);
            margin: 40px 0;
        }
        
        .reference {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            font-size: 0.9em;
            color: #666;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>技术报告 — GAN的博弈论推导（详细版）</h1>
        
        <div class="goal-box">
            <strong>目标：</strong> 以博弈论视角，详细、逐步地推导经典 GAN 的关键结论：判别器最优解 \(D^*(x)\)、把 \(D^*\) 代回后的目标 \(V(D^*,G)\) 与 Jensen–Shannon 散度（JSD）之间的等价关系，以及 Nash 均衡条件。
        </div>
        
        <div class="note">
            本文假设阅读者熟悉概率论的基本概念（期望、密度/质量函数、积分/求和、KL 散度的定义），并愿意跟随逐行代数推导。
        </div>

        <hr>

        <h2>1. 符号与设置（Notation）</h2>
        
        <ul>
            <li>\(x\) 表示样本（例如图像），真实数据分布为 \(p_{\mathrm{data}}(x)\)。</li>
            <li>\(z\) 表示输入给生成器的随机噪声，分布为 \(p_z(z)\)；常见取法为标准正态 \(\mathcal{N}(0,I)\)。</li>
            <li>生成器 \(G\) 将 \(z\) 映射到样本空间： \(x' = G(z)\)。由此诱导的生成分布记为 \(p_g(x)\)。数学上可写为：
                <div class="formula-box">
                    \[ p_g(x) = \int \delta(x - G(z))\,p_z(z)\,dz \]
                </div>
                （对离散情况相应替换为求和）。
            </li>
            <li>判别器 \(D(x)\) 是函数，输出 \(x\) 来自真实分布的概率（或"真"的置信度），其取值在 \((0,1)\)。</li>
        </ul>

        <hr>

        <h2>2. GAN 的原始 min–max 目标（Expectation 形式）</h2>
        
        <p>经典（Goodfellow 等，2014）GAN 的目标为：</p>
        
        <div class="formula-box">
            \begin{equation}
            \min_G \; \max_D \; V(D,G)
            \quad\text{其中}\quad
            V(D,G) = \mathbb{E}_{x \sim p_{\mathrm{data}}}\big[\log D(x)\big]
            + \mathbb{E}_{z \sim p_z}\big[\log\big(1 - D(G(z))\big)\big]
            \tag{1}
            \end{equation}
        </div>

        <ul>
            <li>判别器 \(D\) 想最大化 \(V\)（把真样本判为真，把假样本判为假）。</li>
            <li>生成器 \(G\) 想最小化 \(V\)（生成样本让判别器误判为真）。</li>
        </ul>

        <hr>

        <h2>3. 从期望到积分（或求和）——统一书写形式</h2>
        
        <p>期望可以展开为积分（连续情形）或求和（离散情形）。在连续情形下：</p>
        
        <div class="formula-box">
            \begin{align}
            \mathbb{E}_{x\sim p_{\mathrm{data}}}[\log D(x)] &= \int p_{\mathrm{data}}(x)\,\log D(x)\,dx \\[4pt]
            \mathbb{E}_{z\sim p_z}[\log(1 - D(G(z)))] &= \int p_z(z)\,\log\big(1 - D(G(z))\big)\,dz
            \end{align}
        </div>

        <p>通过变量替换 \(x' = G(z)\)（并将 \(p_z\) pushforward 得到 \(p_g\)），第二项等价于：</p>
        
        <div class="formula-box">
            \[ \int p_g(x)\,\log\big(1 - D(x)\big)\,dx \]
        </div>

        <p>因此，可以把 (1) 写为（对连续情形）：</p>
        
        <div class="result-box">
            \[ V(D,G) = \int \Big[ p_{\mathrm{data}}(x)\log D(x) + p_g(x)\log\big(1 - D(x)\big)\Big]\,dx \tag{2} \]
        </div>

        <div class="note">
            注意：离散情形将积分替换为对 \(x\) 的求和，但推导结构保持一致。
        </div>

        <hr>

        <h2>4. 判别器的逐点最优化（推导 \(D^*(x)\)）</h2>
        
        <p>观察式 (2)。对固定生成器 \(G\)（即固定 \(p_g\)）时，积分是对每个 \(x\) 的逐点贡献的总和，因此可以对每个 \(x\) 单独最大化被积函数：</p>
        
        <div class="formula-box">
            \[ f_x(D(x)) = p_{\mathrm{data}}(x)\log D(x) + p_g(x)\log\big(1 - D(x)\big) \]
        </div>

        <p>对 \(D(x)\in(0,1)\) 求导并令导数为 0：</p>
        
        <div class="formula-box">
            \begin{align}
            \frac{d f_x}{d D(x)} &= \frac{p_{\mathrm{data}}(x)}{D(x)} - \frac{p_g(x)}{1-D(x)} = 0
            \end{align}
        </div>

        <p>解得：</p>
        
        <div class="formula-box">
            \begin{align}
            \frac{p_{\mathrm{data}}(x)}{D(x)} &= \frac{p_g(x)}{1 - D(x)} \\[4pt]
            \Rightarrow\quad p_{\mathrm{data}}(x)\big(1-D(x)\big) &= p_g(x) D(x) \\[4pt]
            \Rightarrow\quad p_{\mathrm{data}}(x) &= \big(p_{\mathrm{data}}(x) + p_g(x)\big)\,D(x)
            \end{align}
        </div>

        <div class="result-box">
            \[ D^*(x) = \frac{p_{\mathrm{data}}(x)}{p_{\mathrm{data}}(x) + p_g(x)} \]
        </div>

        <h3>二阶导检验（确认最大值）：</h3>
        
        <div class="formula-box">
            \[ \frac{d^2 f_x}{d D(x)^2} = -\frac{p_{\mathrm{data}}(x)}{D(x)^2} - \frac{p_g(x)}{(1-D(x))^2} < 0 \]
        </div>

        <p>因此在 \(D^*(x)\) 处是严格的局部（也是全局）最大值（对 \(D(x)\in(0,1)\)）。</p>

        <hr>

        <h2>5. 把 \(D^*(x)\) 代回 \(V(D,G)\) —— 代数展开</h2>
        
        <p>将 \(D^*(x)\) 代入 (2)（把期望写为积分），得到：</p>
        
        <div class="formula-box">
            \begin{align}
            V(D^*,G) &= \int \Big[ p_{\mathrm{data}}(x)\log\frac{p_{\mathrm{data}}(x)}{p_{\mathrm{data}}(x)+p_g(x)} \;
            + \; p_g(x)\log\frac{p_g(x)}{p_{\mathrm{data}}(x)+p_g(x)} \Big]\,dx \\[6pt]
            &= \int \Big[ p_{\mathrm{data}}(x)\log p_{\mathrm{data}}(x) + p_g(x)\log p_g(x) \\
            &\qquad\qquad - \big(p_{\mathrm{data}}(x)+p_g(x)\big)\log\big(p_{\mathrm{data}}(x) + p_g(x)\big) \Big]\,dx \tag{3}
            \end{align}
        </div>

        <div class="note">
            这是代入并把分子、分母的对数拆开的直接结果。下一步引入混合分布 \(m(x)\)。
        </div>

        <hr>

        <h2>6. 引入混合分布 \(m(x)\) 并化成 KL / JSD 形式</h2>
        
        <p>定义混合（mixture）分布：</p>
        
        <div class="formula-box">
            \[ m(x) = \frac{1}{2}\big(p_{\mathrm{data}}(x) + p_g(x)\big) \]
        </div>

        <p>因此 \(p_{\mathrm{data}}(x) + p_g(x) = 2m(x)\)，代入 (3)：</p>
        
        <div class="formula-box">
            \[ V(D^*,G) = \int \Big[ p_{\mathrm{data}}\log p_{\mathrm{data}} + p_g\log p_g - 2m\log(2m) \Big]\,dx \]
        </div>

        <p>利用 \(\log(2m) = \log 2 + \log m\)，得到：</p>
        
        <div class="formula-box">
            \begin{align}
            2m\log(2m) &= 2m\big(\log 2 + \log m\big) = 2m\log 2 + 2m\log m \\[4pt]
            \Rightarrow\quad V(D^*,G) &= \int \Big[ p_{\mathrm{data}}\log p_{\mathrm{data}} - p_{\mathrm{data}}\log m \Big]\,dx \\
            &\quad + \int \Big[ p_g\log p_g - p_g\log m \Big]\,dx - \int 2m\log 2 \,dx
            \end{align}
        </div>

        <p>现在识别出 KL 散度的表达式：对任意概率密度 \(p\) 与 \(q\)，</p>
        
        <div class="formula-box">
            \[ \mathrm{KL}(p\|q) = \int p(x)\log\frac{p(x)}{q(x)}\,dx = \int \big[ p\log p - p\log q \big]\,dx \]
        </div>

        <p>因此：</p>
        
        <div class="formula-box">
            \[ V(D^*,G) = \mathrm{KL}\big(p_{\mathrm{data}}\| m\big) + \mathrm{KL}\big(p_g \| m\big) - \int 2m(x)\log 2 \,dx \]
        </div>

        <p>因为 \(m\) 为概率密度，\(\int m(x)\,dx = 1\)，所以 \(\int 2m(x)\log 2 \,dx = 2\log 2\)。</p>

        <div class="result-box">
            \[ V(D^*,G) = \mathrm{KL}(p_{\mathrm{data}}\|m) + \mathrm{KL}(p_g\|m) - 2\log 2 \]
        </div>

        <hr>

        <h2>7. 由上述式子得到 JSD 的表示</h2>
        
        <p>Jensen–Shannon 散度定义为：</p>
        
        <div class="formula-box">
            \[ \mathrm{JSD}(p_{\mathrm{data}}\|p_g) = \frac{1}{2}\mathrm{KL}(p_{\mathrm{data}}\|m) + \frac{1}{2}\mathrm{KL}(p_g\|m) \]
        </div>

        <p>乘以 2 并代回：</p>
        
        <div class="formula-box">
            \[ 2\,\mathrm{JSD}(p_{\mathrm{data}}\|p_g) = \mathrm{KL}(p_{\mathrm{data}}\|m) + \mathrm{KL}(p_g\|m) \]
        </div>

        <div class="result-box">
            \[ V(D^*,G) = -\log 4 + 2\,\mathrm{JSD}(p_{\mathrm{data}}\|p_g) \]
            <div style="margin-top: 10px; font-size: 0.9em;">
                （注：\(-2\log 2 = -\log 4\)）
            </div>
        </div>

        <hr>

        <h2>8. Nash 均衡与含义</h2>
        
        <ul>
            <li>JSD 总是非负：\(\mathrm{JSD}\ge 0\)。当且仅当 \(p_{\mathrm{data}} = p_g\)（几乎处处）时取 0。</li>
            <li>从上式可见，固定 \(D^*\) 的情况下，生成器在最小化 \(V(D^*,G)\) 等价于最小化 \(\mathrm{JSD}(p_{\mathrm{data}}\|p_g)\)。</li>
            <li>因此 Nash 均衡（两方互为最佳响应）发生在：
                <div class="result-box">
                    \[ p_g = p_{\mathrm{data}}, \quad\text{此时}\quad V(D^*,G) = -\log 4 \]
                </div>
                同时 \(D^*(x) = \frac{1}{2}\)（几乎处处），因为 \(p_{\mathrm{data}}(x) = p_g(x)\Rightarrow D^*(x) = 1/2\)。
            </li>
        </ul>

        <hr>

        <h2>9. 细节、数值离散示例与核查（验证代数）</h2>
        
        <div class="example-box">
            <h3>离散二点示例（简化核查）</h3>
            <p>样本空间 \(\{a,b\}\)。取：</p>
            \begin{align}
            p_{\mathrm{data}}(a) &= 0.8,\quad p_{\mathrm{data}}(b)=0.2 \\
            p_g(a) &= 0.5,\quad p_g(b)=0.5
            \end{align}
            <p>混合分布 \(m = \frac{1}{2}(p_{\mathrm{data}}+p_g)=(0.65,0.35)\)。</p>
            
            <p><strong>计算验证：</strong></p>
            <ul>
                <li>直接使用公式 (3)：对每个点计算 \(p_{\mathrm{data}}\log p_{\mathrm{data}} + p_g\log p_g - (p_{\mathrm{data}}+p_g)\log(p_{\mathrm{data}}+p_g)\)，再求和。</li>
                <li>或计算 \(\mathrm{KL}(p_{\mathrm{data}}\|m) + \mathrm{KL}(p_g\|m) - \log 4\)。</li>
            </ul>
            <p>两种方法会给出相同数值，验证代数步骤无误。</p>
        </div>

        <hr>

        <h2>10. 关于 \(p_g(x)=\int \delta(x-G(z))p_z(z)\,dz\) 的补充解释</h2>
        
        <ul>
            <li>这只是数学上把"在\(p_z\) 下采样 \(z\)，经过函数 \(G\) 后得到的 \(x\)"写成密度的标准做法（pushforward measure）。</li>
            <li>\(\delta(\cdot)\) 的作用是"把 \(z\)-空间的概率质量映射到对应的 \(x\)-点"，满足：
                <div class="formula-box">
                    \[ \int f(x)\,\delta(x-G(z))\,dx = f(G(z)) \]
                </div>
            </li>
            <li>若 \(G\) 可逆且光滑，则有更常见的密度变换公式；但在深度网络里 \(G\) 常不可逆，所以用 δ 表示通用的 pushforward。</li>
        </ul>

        <hr>

        <h2>11. 实务与局限（简要）</h2>
        
        <ul>
            <li><strong>参数化 / 近似最优：</strong>推导中使用了 \(D^*\) 的闭式解，假设判别器可以达到最优；在实际训练中，判别器是参数化网络且只能被近似优化（有限步骤、有限容量），因此实际行为可能偏离理论轨迹。</li>
            
            <li><strong>训练不稳定性：</strong>GAN 训练常见问题包括梯度不稳定、模式崩溃（mode collapse）、震荡等；理论最优条件提供了目标形式，但并不保证优化过程收敛。</li>
            
            <li><strong>替代损失：</strong>为了稳定训练，实践中常用非饱和损失（heuristic），例如生成器直接最大化 \(\mathbb{E}_{z}[\log D(G(z))]\)（这在某些阶段会有更稳定的梯度）。</li>
            
            <li><strong>度量选择：</strong>JSD 是一种对称的距离度量（基于 KL），但在高维稀疏支撑情况下它对非重叠分布的梯度可能很差；基于此出现了 Wasserstein GAN 等变体。</li>
        </ul>

        <hr>

        <h2>12. 结论</h2>
        
        <div class="highlight-box">
            <ul>
                <li>经典 GAN 的 min–max 目标可以逐点优化得到判别器的闭式最优解 \(D^*(x)\)。</li>
                <li>把 \(D^*\) 代回目标后会自然出现 KL 项并组合成 Jensen–Shannon 散度：
                    <div class="result-box" style="margin: 15px 0;">
                        \[ V(D^*,G) = -\log 4 + 2\,\mathrm{JSD}(p_{\mathrm{data}}\|p_g) \]
                    </div>
                </li>
                <li>Nash 均衡对应于 \(p_g = p_{\mathrm{data}}\)，此时 JSD = 0，判别器输出 \(1/2\)。</li>
            </ul>
        </div>

        <hr>

        <div class="reference">
            <h3>参考文献</h3>
            <p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks. arXiv:1406.2661.</p>
        </div>
    </div>
</body>
</html>