import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

# ============================================================================
# 1. KL Annealing å®ç°
# ============================================================================

class KLAnnealingScheduler:
    """KLæƒé‡é€€ç«è°ƒåº¦å™¨"""
    def __init__(self, anneal_type='linear', max_steps=10000, min_beta=0.0, max_beta=1.0):
        self.anneal_type = anneal_type
        self.max_steps = max_steps
        self.min_beta = min_beta
        self.max_beta = max_beta
    
    def get_beta(self, step):
        if step >= self.max_steps:
            return self.max_beta
        
        progress = step / self.max_steps
        
        if self.anneal_type == 'linear':
            beta = self.min_beta + (self.max_beta - self.min_beta) * progress
        
        elif self.anneal_type == 'cosine':
            beta = self.min_beta + (self.max_beta - self.min_beta) * 0.5 * (1 + math.cos(math.pi * (1 - progress)))
        
        elif self.anneal_type == 'sigmoid':
            # sigmoid annealing: æ…¢å¯åŠ¨ï¼Œå¿«ç»“æŸ
            s = 1.0 / (1.0 + math.exp(-12 * (progress - 0.5)))
            beta = self.min_beta + (self.max_beta - self.min_beta) * s
        
        elif self.anneal_type == 'cyclical':
            # å¾ªç¯é€€ç«ï¼Œé€‚åˆé•¿è®­ç»ƒ
            cycle = 4  # æ¯ä¸ªå‘¨æœŸçš„æ¯”ä¾‹
            progress_in_cycle = (progress * cycle) % 1.0
            beta = self.min_beta + (self.max_beta - self.min_beta) * progress_in_cycle
        
        return beta

class KLAnnealingVAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super().__init__()
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
        
        self.kl_scheduler = KLAnnealingScheduler('sigmoid', max_steps=5000)
        self.current_step = 0
    
    def encode(self, x):
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        
        # KLæ•£åº¦
        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
        
        # é‡å»ºæŸå¤±
        recon_loss = F.binary_cross_entropy(recon, x, reduction='none').sum(dim=1)
        
        # åŠ¨æ€Î²æƒé‡
        beta = self.kl_scheduler.get_beta(self.current_step)
        
        # æ€»æŸå¤±
        loss = recon_loss + beta * kl
        
        return {
            'loss': loss.mean(),
            'recon_loss': recon_loss.mean(),
            'kl_loss': kl.mean(),
            'beta': beta,
            'recon': recon
        }
    
    def step(self):
        """è®­ç»ƒæ­¥æ•°æ›´æ–°"""
        self.current_step += 1

# ============================================================================
# 2. Free Bits å®ç°
# ============================================================================

class FreeBitsVAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim, free_bits=2.0):
        super().__init__()
        self.free_bits = free_bits  # æ¯ä¸ªç»´åº¦çš„å…è´¹bits
        
        # ç½‘ç»œç»“æ„åŒä¸Š
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def free_bits_kl(self, mu, logvar):
        """Free bits KLæ•£åº¦è®¡ç®—"""
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„KLæ•£åº¦
        kl_per_dim = 0.5 * (mu.pow(2) + logvar.exp() - logvar - 1)
        
        # åº”ç”¨free bitsï¼šmax(free_bits, kl_per_dim)
        kl_free = torch.clamp(kl_per_dim, min=self.free_bits)
        
        return kl_free.sum(dim=1)  # å¯¹latentç»´åº¦æ±‚å’Œ
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        
        # Free bits KL
        kl = self.free_bits_kl(mu, logvar)
        
        # é‡å»ºæŸå¤±
        recon_loss = F.binary_cross_entropy(recon, x, reduction='none').sum(dim=1)
        
        # æ€»æŸå¤±
        loss = recon_loss + kl
        
        # è®¡ç®—å®é™…ä½¿ç”¨çš„bitsæ•°
        kl_per_dim = 0.5 * (mu.pow(2) + logvar.exp() - logvar - 1)
        active_dims = (kl_per_dim > self.free_bits).sum(dim=1).float()
        
        return {
            'loss': loss.mean(),
            'recon_loss': recon_loss.mean(),
            'kl_loss': kl.mean(),
            'active_dims': active_dims.mean(),
            'recon': recon
        }

# ============================================================================
# 3. Hierarchical VAE å®ç°
# ============================================================================

class HierarchicalVAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dims=[32, 16, 8]):
        super().__init__()
        self.latent_dims = latent_dims
        self.n_levels = len(latent_dims)
        
        # ç¼–ç å™¨ï¼šbottom-up
        self.encoders = nn.ModuleList()
        prev_dim = input_dim
        for i, latent_dim in enumerate(latent_dims):
            encoder = nn.Sequential(
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU()
            )
            self.encoders.append(encoder)
            prev_dim = hidden_dim
        
        # ç¼–ç å™¨è¾“å‡ºå±‚
        self.mu_layers = nn.ModuleList([
            nn.Linear(hidden_dim, dim) for dim in latent_dims
        ])
        self.logvar_layers = nn.ModuleList([
            nn.Linear(hidden_dim, dim) for dim in latent_dims
        ])
        
        # å…ˆéªŒç½‘ç»œï¼štop-down
        self.prior_nets = nn.ModuleList()
        for i in range(self.n_levels - 1):
            # æ¯å±‚çš„å…ˆéªŒä¾èµ–äºä¸Šå±‚
            prior_net = nn.Sequential(
                nn.Linear(latent_dims[i+1], hidden_dim//2),
                nn.ReLU(),
                nn.Linear(hidden_dim//2, latent_dims[i] * 2)  # mu + logvar
            )
            self.prior_nets.append(prior_net)
        
        # è§£ç å™¨ï¼štop-down
        self.decoder = nn.Sequential(
            nn.Linear(sum(latent_dims), hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        """Bottom-up encoding"""
        mus, logvars = [], []
        h = x
        
        for i in range(self.n_levels):
            h = self.encoders[i](h)
            mu = self.mu_layers[i](h)
            logvar = self.logvar_layers[i](h)
            mus.append(mu)
            logvars.append(logvar)
        
        return mus, logvars
    
    def get_prior_params(self, zs):
        """è·å–æ¡ä»¶å…ˆéªŒå‚æ•°"""
        prior_mus, prior_logvars = [], []
        
        # æœ€é¡¶å±‚ä½¿ç”¨æ ‡å‡†æ­£æ€å…ˆéªŒ
        prior_mus.append(torch.zeros_like(zs[-1]))
        prior_logvars.append(torch.zeros_like(zs[-1]))
        
        # å…¶ä»–å±‚ä½¿ç”¨æ¡ä»¶å…ˆéªŒ
        for i in range(self.n_levels - 2, -1, -1):
            prior_params = self.prior_nets[i](zs[i+1])
            mu_dim = self.latent_dims[i]
            prior_mu = prior_params[:, :mu_dim]
            prior_logvar = prior_params[:, mu_dim:]
            prior_mus.append(prior_mu)
            prior_logvars.append(prior_logvar)
        
        # ç¿»è½¬é¡ºåºä»¥åŒ¹é…ç¼–ç å™¨è¾“å‡º
        return list(reversed(prior_mus)), list(reversed(prior_logvars))
    
    def reparameterize(self, mus, logvars):
        zs = []
        for mu, logvar in zip(mus, logvars):
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + eps * std
            zs.append(z)
        return zs
    
    def decode(self, zs):
        # è¿æ¥æ‰€æœ‰å±‚çš„æ½œå˜é‡
        z_concat = torch.cat(zs, dim=1)
        return self.decoder(z_concat)
    
    def forward(self, x):
        # ç¼–ç 
        post_mus, post_logvars = self.encode(x)
        zs = self.reparameterize(post_mus, post_logvars)
        
        # è·å–å…ˆéªŒå‚æ•°
        prior_mus, prior_logvars = self.get_prior_params(zs)
        
        # è§£ç 
        recon = self.decode(zs)
        
        # è®¡ç®—å±‚æ¬¡KLæ•£åº¦
        kl_losses = []
        for i in range(self.n_levels):
            # æ¯å±‚çš„KL: KL(q(z_i|z_{i+1}, x) || p(z_i|z_{i+1}))
            kl = 0.5 * torch.sum(
                prior_logvars[i] - post_logvars[i] +
                (post_logvars[i].exp() + (post_mus[i] - prior_mus[i]).pow(2)) / 
                prior_logvars[i].exp() - 1,
                dim=1
            )
            kl_losses.append(kl)
        
        total_kl = sum(kl_losses)
        
        # é‡å»ºæŸå¤±
        recon_loss = F.binary_cross_entropy(recon, x, reduction='none').sum(dim=1)
        
        # æ€»æŸå¤±
        loss = recon_loss + total_kl
        
        return {
            'loss': loss.mean(),
            'recon_loss': recon_loss.mean(),
            'kl_loss': total_kl.mean(),
            'kl_per_level': [kl.mean().item() for kl in kl_losses],
            'recon': recon
        }

# ============================================================================
# 4. VQ-VAE å®ç°
# ============================================================================

class VectorQuantizer(nn.Module):
    """å‘é‡é‡åŒ–å±‚"""
    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings
        self.commitment_cost = commitment_cost
        
        # åˆ›å»ºembeddingè¡¨
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)
    
    def forward(self, inputs):
        # inputs: [B, embedding_dim]
        
        # è®¡ç®—è·ç¦»
        flat_input = inputs.view(-1, self.embedding_dim)
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                    + torch.sum(self.embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self.embedding.weight.t()))
        
        # æ‰¾åˆ°æœ€è¿‘çš„embedding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        
        # é‡åŒ–
        quantized = torch.matmul(encodings, self.embedding.weight)
        quantized = quantized.view_as(inputs)
        
        # è®¡ç®—æŸå¤±
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self.commitment_cost * e_latent_loss
        
        # Straight through estimator
        quantized = inputs + (quantized - inputs).detach()
        
        return quantized, loss, encoding_indices.view(-1)

class VQVAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, embedding_dim, num_embeddings):
        super().__init__()
        
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        
        # å‘é‡é‡åŒ–
        self.vq = VectorQuantizer(num_embeddings, embedding_dim)
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # ç¼–ç 
        z_e = self.encoder(x)
        
        # é‡åŒ–
        z_q, vq_loss, encoding_indices = self.vq(z_e)
        
        # è§£ç 
        recon = self.decoder(z_q)
        
        # é‡å»ºæŸå¤±
        recon_loss = F.binary_cross_entropy(recon, x, reduction='none').sum(dim=1)
        
        # æ€»æŸå¤±
        loss = recon_loss + vq_loss
        
        # è®¡ç®—codebookä½¿ç”¨ç‡
        unique_encodings = len(torch.unique(encoding_indices))
        codebook_usage = unique_encodings / self.vq.num_embeddings
        
        return {
            'loss': loss.mean(),
            'recon_loss': recon_loss.mean(),
            'vq_loss': vq_loss,
            'codebook_usage': codebook_usage,
            'encoding_indices': encoding_indices,
            'recon': recon
        }

# ============================================================================
# 5. è®­ç»ƒè„šæœ¬ç¤ºä¾‹
# ============================================================================

def train_vae(model, dataloader, num_epochs=100, device='cuda'):
    """é€šç”¨VAEè®­ç»ƒå‡½æ•°"""
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_idx, (data, _) in enumerate(dataloader):
            data = data.to(device)
            data = data.view(data.size(0), -1)  # flatten
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(data)
            loss = outputs['loss']
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # æ›´æ–°æ­¥æ•°ï¼ˆKL Annealingéœ€è¦ï¼‰
            if hasattr(model, 'step'):
                model.step()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
                
                # æ‰“å°é¢å¤–ä¿¡æ¯
                if 'beta' in outputs:
                    print(f'  Beta: {outputs["beta"]:.4f}')
                if 'active_dims' in outputs:
                    print(f'  Active dims: {outputs["active_dims"]:.1f}')
                if 'codebook_usage' in outputs:
                    print(f'  Codebook usage: {outputs["codebook_usage"]:.2f}')
        
        avg_loss = total_loss / len(dataloader)
        print(f'Epoch {epoch} completed. Average loss: {avg_loss:.4f}')

# ============================================================================
# 7. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ - å¯¹æ¯”æ‰€æœ‰æ–¹æ³•
# ============================================================================

if __name__ == "__main__":
    # ç”Ÿæˆæ›´çœŸå®çš„æµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹ŸMNISTï¼‰
    torch.manual_seed(42)
    np.random.seed(42)
    
    def create_synthetic_data():
        """åˆ›å»ºåˆæˆæ•°æ®é›†ï¼Œæ¨¡æ‹Ÿä¸åŒç±»åˆ«"""
        n_samples = 2000
        n_features = 784
        n_classes = 10
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºä¸åŒçš„æ¨¡å¼
        X = []
        y = []
        for class_id in range(n_classes):
            # æ¯ä¸ªç±»åˆ«çš„åŸºç¡€æ¨¡å¼
            base_pattern = torch.randn(n_features) * 0.5
            
            # è¯¥ç±»åˆ«çš„æ ·æœ¬
            for _ in range(n_samples // n_classes):
                sample = base_pattern + torch.randn(n_features) * 0.3
                sample = torch.sigmoid(sample)  # å½’ä¸€åŒ–åˆ°[0,1]
                X.append(sample)
                y.append(class_id)
        
        return torch.stack(X), torch.tensor(y)
    
    print("ğŸ”§ ç”Ÿæˆåˆæˆæ•°æ®é›†...")
    X, y = create_synthetic_data()
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    train_size = int(0.8 * len(X))
    train_X, train_y = X[:train_size], y[:train_size]
    test_X, test_y = X[train_size:], y[train_size:]
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader, TensorDataset
    
    train_dataset = TensorDataset(train_X, train_y)
    test_dataset = TensorDataset(test_X, test_y)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: è®­ç»ƒ={len(train_X)}, æµ‹è¯•={len(test_X)}")
    
    # å®šä¹‰æ‰€æœ‰è¦æµ‹è¯•çš„æ¨¡å‹
    models_to_test = {
        'KL Annealing VAE': KLAnnealingVAE(784, 400, 20),
        'Free Bits VAE': FreeBitsVAE(784, 400, 20, free_bits=2.0),
        'Hierarchical VAE': HierarchicalVAE(784, 400, [32, 16, 8]),
        'VQ-VAE': VQVAE(784, 400, 64, 512)
    }
    
    # è®¾å¤‡é€‰æ‹©
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®­ç»ƒæ¯ä¸ªæ¨¡å‹å¹¶å¯¹æ¯”æ•ˆæœ
    results = {}
    
    print("\n" + "="*80)
    print("ğŸ å¼€å§‹å¯¹æ¯”è®­ç»ƒä¸åŒVAEæ–¹æ³•")
    print("="*80)
    
    for model_name, model in models_to_test.items():
        print(f"\n{'ğŸš€ ' + model_name + ' ':=^60}")
        
        # è®­ç»ƒæ¨¡å‹
        metrics = train_vae_with_monitoring(
            model=model,
            train_loader=train_loader, 
            test_loader=test_loader,
            num_epochs=15,  # è¾ƒå°‘epochä¾¿äºå¿«é€Ÿæµ‹è¯•
            device=device,
            model_name=model_name
        )
        
        results[model_name] = {
            'model': model,
            'metrics': metrics,
            'final_loss': metrics['total_loss'][-1],
            'final_recon': metrics['recon_loss'][-1],
            'final_kl': metrics['kl_loss'][-1]
        }
        
        print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ\n")
    
    # ============================================================================
    # 8. æœ€ç»ˆå¯¹æ¯”åˆ†æ
    # ============================================================================
    
    print("\n" + "="*80)
    print("ğŸ“Š æ‰€æœ‰æ–¹æ³•çš„æœ€ç»ˆå¯¹æ¯”åˆ†æ")
    print("="*80)
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    print(f"{'æ–¹æ³•':<20} {'æ€»æŸå¤±':<10} {'é‡å»ºæŸå¤±':<10} {'KLæŸå¤±':<10} {'è¯„ä»·'}")
    print("-" * 70)
    
    for name, result in results.items():
        final_loss = result['final_loss']
        final_recon = result['final_recon']  
        final_kl = result['final_kl']
        
        # ç®€å•çš„æ•ˆæœè¯„ä»·
        if final_recon < 0.1 and final_kl < 5.0:
            evaluation = "ğŸŸ¢ ä¼˜ç§€"
        elif final_recon < 0.2 and final_kl < 10.0:
            evaluation = "ğŸŸ¡ è‰¯å¥½"  
        else:
            evaluation = "ğŸ”´ éœ€æ”¹è¿›"
            
        print(f"{name:<20} {final_loss:<10.4f} {final_recon:<10.4f} {final_kl:<10.4f} {evaluation}")
    
    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_method = min(results.keys(), key=lambda x: results[x]['final_loss'])
    print(f"\nğŸ† ç»¼åˆè¡¨ç°æœ€ä½³: {best_method}")
    
    # è¯¦ç»†åˆ†ææœ€ä½³æ–¹æ³•
    print(f"\nğŸ” {best_method} è¯¦ç»†åˆ†æ:")
    best_model = results[best_method]['model']
    test_data_sample = test_X[:100].to(device)
    check_posterior_collapse(best_model, test_data_sample, best_method)
    
    print("\n" + "="*80)
    print("ğŸ¯ è®­ç»ƒå»ºè®®æ€»ç»“")
    print("="*80)
    
    print("""
    æ ¹æ®è®­ç»ƒç»“æœï¼Œä»¥ä¸‹æ˜¯é€‰æ‹©å»ºè®®:
    
    ğŸ¥‡ KL Annealing VAE:
       - å®ç°ç®€å•ï¼Œæ•ˆæœç¨³å®š
       - é€‚åˆ: åˆå­¦è€…ï¼Œå¿«é€ŸåŸå‹
       - æ³¨æ„: éœ€è¦è°ƒèŠ‚é€€ç«ç­–ç•¥
    
    ğŸ¥ˆ Free Bits VAE:
       - è¶…å‚æ•°å®¹æ˜“è°ƒèŠ‚
       - é€‚åˆ: å¯¹ç»´åº¦åˆ©ç”¨ç‡æœ‰è¦æ±‚
       - æ³¨æ„: free_bitsè®¾ç½®å¾ˆé‡è¦
    
    ğŸ¥‰ Hierarchical VAE:
       - è¡¨è¾¾èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤æ‚æ•°æ®
       - é€‚åˆ: å±‚æ¬¡ç»“æ„æ˜æ˜¾çš„æ•°æ®
       - æ³¨æ„: è®¡ç®—å¤æ‚åº¦è¾ƒé«˜
    
    ğŸ… VQ-VAE:
       - æ ¹æœ¬è§£å†³posterior collapse
       - é€‚åˆ: å¯¹ç”Ÿæˆè´¨é‡è¦æ±‚é«˜
       - æ³¨æ„: éœ€è¦è°ƒèŠ‚codebookå¤§å°
    """)
    
    print("\nâœ¨ å®éªŒå®Œæˆ! ä½ å¯ä»¥æ ¹æ®ç»“æœé€‰æ‹©æœ€é€‚åˆä½ æ•°æ®çš„æ–¹æ³•ã€‚")

