from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½ Mistral-7B-Instruct æ¨¡å‹
model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
model.eval()

# åˆå§‹åŒ–ä¸Šä¸‹æ–‡è®°å¿†åˆ—è¡¨
context_history = []

# æ·»åŠ ç”¨æˆ·è¾“å…¥åå¥½
user_input = "I like suspenseful movies with tight plots, preferably released in recent years."
context_history.append(f"User: {user_input}")

# æ„é€ åˆå§‹ prompt
def build_prompt(context):
    return "\n".join(context + ["Assistant: Recommend 5 movies and provide reasons.\n1."])

# æ¨ç†å‡½æ•°
def generate_recommendation(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=512,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ç¬¬1è½®æ¨è
prompt_1 = build_prompt(context_history)
response_1 = generate_recommendation(prompt_1)
context_history.append("Assistant: " + response_1.split("Assistant:")[-1].strip())

print("\u2728 Round 1 Recommendation:")
print(response_1)

# æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆ
user_feedback = "I've already watched some of those. Can you recommend lesser-known titles with similar suspense?"
context_history.append(f"User: {user_feedback}")

# ç¬¬2è½®æ¨è
prompt_2 = build_prompt(context_history)
response_2 = generate_recommendation(prompt_2)
context_history.append("Assistant: " + response_2.split("Assistant:")[-1].strip())

print("\nğŸ”„ Round 2 Recommendation:")
print(response_2)

# ä½ å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šè½®æ¬¡çš„ç”¨æˆ·è¾“å…¥å¹¶è°ƒç”¨ç›¸åŒé€»è¾‘è¿›è¡Œå¤šè½®ç”Ÿæˆ
