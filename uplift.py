import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

print("ğŸ”„ é‡æ–°è¿è¡Œä¼˜åŒ–ç‰ˆæœ¬...")

# ========================
# 1. æ•°æ®ç”Ÿæˆï¼ˆä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ä¿è¯ç»“æœä¸€è‡´ï¼‰
# ========================

def generate_coupon_data(n_samples=100000, random_state=42):
    np.random.seed(random_state)
    
    data = {
        'user_id': range(n_samples),
        'age': np.random.normal(35, 10, n_samples).astype(int),
        'income': np.random.lognormal(10, 0.5, n_samples),
        'historical_purchases': np.random.poisson(5, n_samples),
        'price_sensitivity': np.random.beta(2, 5, n_samples),
        'activity_level': np.random.beta(2, 2, n_samples),
        'last_purchase_days': np.random.exponential(30, n_samples).astype(int)
    }
    
    df = pd.DataFrame(data)
    df['age'] = np.clip(df['age'], 18, 70)
    df['last_purchase_days'] = np.clip(df['last_purchase_days'], 1, 365)
    df['treatment'] = np.random.binomial(1, 0.5, n_samples)
    
    # è´­ä¹°æ¦‚ç‡æ¨¡å‹
    base_prob = (0.1 + 0.2 * (df['historical_purchases'] / df['historical_purchases'].max()) +
                0.1 * (1 - df['price_sensitivity']) + 0.05 * df['activity_level'] -
                0.001 * df['last_purchase_days'])
    
    treatment_effect = 0.3 * df['price_sensitivity'] * df['treatment']
    purchase_prob = np.clip(base_prob + treatment_effect, 0.05, 0.95)
    df['purchase'] = np.random.binomial(1, purchase_prob, n_samples)
    df['true_uplift'] = 0.3 * df['price_sensitivity']
    
    return df

# ç”Ÿæˆæ•°æ®
df = generate_coupon_data(100000)
print("âœ… æ•°æ®ç”Ÿæˆå®Œæˆ")

# ========================
# 2. ä¼˜åŒ–å»ºæ¨¡éƒ¨åˆ†
# ========================

feature_cols = ['age', 'income', 'historical_purchases', 'price_sensitivity', 
                'activity_level', 'last_purchase_days']
X = df[feature_cols]
T = df['treatment']
Y = df['purchase']

X_train, X_test, T_train, T_test, Y_train, Y_test = train_test_split(
    X, T, Y, test_size=0.3, random_state=42, stratify=T
)

# 2.1 åŒæ¨¡å‹æ³•
print("ğŸ¤– è®­ç»ƒåŒæ¨¡å‹æ³•...")
model_control = RandomForestClassifier(n_estimators=100, random_state=42)
model_treatment = RandomForestClassifier(n_estimators=100, random_state=42)

model_control.fit(X_train[T_train == 0], Y_train[T_train == 0])
model_treatment.fit(X_train[T_train == 1], Y_train[T_train == 1])

uplift_tm = model_treatment.predict_proba(X_test)[:, 1] - model_control.predict_proba(X_test)[:, 1]

# 2.2 æ”¹è¿›çš„ç®€åŒ–Causal Forest
print("ğŸ¤– è®­ç»ƒæ”¹è¿›çš„Causal Forest...")

# ä½¿ç”¨æ›´ç¨³å¥çš„æ–¹æ³•
X_train_extended = X_train.copy()
X_train_extended['treatment'] = T_train

# æ·»åŠ treatmentä¸ç‰¹å¾çš„äº¤äº’é¡¹
for col in feature_cols:
    X_train_extended[f'treatment_x_{col}'] = T_train * X_train[col]

model_cf = RandomForestRegressor(n_estimators=100, random_state=42)
model_cf.fit(X_train_extended, Y_train)

# é¢„æµ‹uplift
X_test_t1 = X_test.copy()
X_test_t1['treatment'] = 1
for col in feature_cols:
    X_test_t1[f'treatment_x_{col}'] = 1 * X_test[col]

X_test_t0 = X_test.copy()
X_test_t0['treatment'] = 0
for col in feature_cols:
    X_test_t0[f'treatment_x_{col}'] = 0 * X_test[col]

uplift_cf = model_cf.predict(X_test_t1) - model_cf.predict(X_test_t0)

# ========================
# 3. ä¼˜åŒ–å¯è§†åŒ–
# ========================

print("ğŸ“Š ç”Ÿæˆä¼˜åŒ–åçš„å¯è§†åŒ–...")

# 3.1 Upliftåˆ†å¸ƒç›´æ–¹å›¾ - ä¿®å¤æ˜¾ç¤ºé—®é¢˜
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# åŒæ¨¡å‹æ³•Upliftåˆ†å¸ƒ
axes[0].hist(uplift_tm, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
axes[0].set_title('åŒæ¨¡å‹æ³• - Upliftåˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[0].set_xlabel('é¢„æµ‹Upliftå€¼')
axes[0].set_ylabel('ç”¨æˆ·æ•°é‡')
axes[0].grid(True, alpha=0.3)

# Causal Forest Upliftåˆ†å¸ƒ
axes[1].hist(uplift_cf, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
axes[1].set_title('Causal Forest - Upliftåˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[1].set_xlabel('é¢„æµ‹Upliftå€¼')
axes[1].set_ylabel('ç”¨æˆ·æ•°é‡')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('uplift_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# 3.2 æ”¹è¿›çš„Qiniæ›²çº¿
def plot_improved_qini_curve(uplift_scores, actual_purchase, treatment, method_name):
    """æ”¹è¿›çš„Qiniæ›²çº¿ç»˜åˆ¶"""
    # æŒ‰upliftåˆ†æ•°é™åºæ’åˆ—
    sorted_indices = np.argsort(uplift_scores)[::-1]
    sorted_treatment = treatment.iloc[sorted_indices].values
    sorted_purchase = actual_purchase.iloc[sorted_indices].values
    
    n_samples = len(sorted_indices)
    x_percent = np.arange(1, n_samples + 1) / n_samples * 100
    
    # è®¡ç®—ç´¯ç§¯è½¬åŒ–ç‡
    cumulative_treated = np.cumsum(sorted_treatment)
    cumulative_purchases = np.cumsum(sorted_purchase * sorted_treatment)
    
    # è®¡ç®—è½¬åŒ–ç‡
    conversion_rates = np.zeros(n_samples)
    for i in range(n_samples):
        if cumulative_treated[i] > 0:
            conversion_rates[i] = cumulative_purchases[i] / cumulative_treated[i]
        else:
            conversion_rates[i] = 0
    
    # éšæœºåŸºå‡†çº¿
    overall_conversion = actual_purchase.mean()
    random_conversion = np.cumsum([overall_conversion] * n_samples)
    random_rates = random_conversion / np.arange(1, n_samples + 1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(x_percent, conversion_rates * 100, 
             label=f'{method_name}ç­–ç•¥', linewidth=2, color='red')
    plt.plot(x_percent, random_rates * 100, 
             label='éšæœºç­–ç•¥', linestyle='--', linewidth=2, color='blue')
    
    plt.xlabel('ç›®æ ‡ç”¨æˆ·ç™¾åˆ†æ¯” (%)', fontsize=12)
    plt.ylabel('è½¬åŒ–ç‡ (%)', fontsize=12)
    plt.title(f'{method_name} - Qiniæ›²çº¿', fontsize=14, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xlim(0, 100)
    plt.ylim(0, max(conversion_rates.max(), overall_conversion) * 100 * 1.1)
    
    # æ·»åŠ é¢ç§¯å¡«å……
    plt.fill_between(x_percent, conversion_rates * 100, random_rates * 100, 
                     alpha=0.2, color='green')
    
    plt.tight_layout()
    plt.savefig(f'qini_curve_{method_name}.png', dpi=300, bbox_inches='tight')
    plt.show()

# ç»˜åˆ¶Qiniæ›²çº¿
plot_improved_qini_curve(uplift_tm, Y_test, T_test, "åŒæ¨¡å‹æ³•")
plot_improved_qini_curve(uplift_cf, Y_test, T_test, "Causal_Forest")

# 3.3 ç‰¹å¾é‡è¦æ€§åˆ†æ
print("ğŸ” åˆ†æç‰¹å¾é‡è¦æ€§...")

# ä»æ¨¡å‹ä¸­è·å–ç‰¹å¾é‡è¦æ€§
feature_importance_cf = model_cf.feature_importances_
feature_names = list(X_train_extended.columns)

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance_cf
}).sort_values('importance', ascending=True)

# åªæ˜¾ç¤ºå‰15ä¸ªæœ€é‡è¦çš„ç‰¹å¾
top_features = importance_df.tail(15)

plt.figure(figsize=(12, 8))
plt.barh(top_features['feature'], top_features['importance'])
plt.xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°', fontsize=12)
plt.title('Causal Forest - ç‰¹å¾é‡è¦æ€§æ’å (Top 15)', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

# 3.4 ç­–ç•¥æ•ˆæœå¯¹æ¯”å›¾
def plot_strategy_comparison(uplift_tm, uplift_cf, Y_test, T_test):
    """ç­–ç•¥æ•ˆæœå¯¹æ¯”å›¾"""
    methods = ['åŒæ¨¡å‹æ³•', 'Causal Forest']
    uplift_scores = [uplift_tm, uplift_cf]
    
    results = []
    for i, (method, uplift) in enumerate(zip(methods, uplift_scores)):
        n_target = int(len(uplift) * 0.3)
        target_indices = np.argsort(uplift)[-n_target:]
        
        # è®¡ç®—ç­–ç•¥æ•ˆæœ
        uplift_roi = Y_test.iloc[target_indices].mean()
        current_roi = Y_test.mean()
        improvement = (uplift_roi - current_roi) / current_roi * 100
        
        results.append({
            'Method': method,
            'Current_ROI': current_roi,
            'Uplift_ROI': uplift_roi,
            'Improvement': improvement
        })
    
    results_df = pd.DataFrame(results)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    x = np.arange(len(methods))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, results_df['Current_ROI'], width, 
                   label='å½“å‰ç­–ç•¥', alpha=0.7, color='lightblue')
    bars2 = ax.bar(x + width/2, results_df['Uplift_ROI'], width, 
                   label='Upliftç­–ç•¥', alpha=0.7, color='lightcoral')
    
    ax.set_xlabel('å»ºæ¨¡æ–¹æ³•', fontsize=12)
    ax.set_ylabel('è½¬åŒ–ç‡', fontsize=12)
    ax.set_title('ç­–ç•¥æ•ˆæœå¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(methods)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.3f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),
                       textcoords="offset points",
                       ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('strategy_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return results_df

# ç»˜åˆ¶ç­–ç•¥å¯¹æ¯”
results_df = plot_strategy_comparison(uplift_tm, uplift_cf, Y_test, T_test)

# ========================
# 4. æœ€ç»ˆæŠ¥å‘Š
# ========================

print("\n" + "="*60)
print("ğŸ‰ ä¼˜åŒ–ç‰ˆé¡¹ç›®æ€»ç»“æŠ¥å‘Š")
print("="*60)

print(f"\nğŸ“Š æ•°æ®é›†æ¦‚å†µ:")
print(f"â€¢ æ€»ç”¨æˆ·æ•°: {len(df):,}")
print(f"â€¢ å‘åˆ¸ç»„è½¬åŒ–ç‡: {df[df['treatment']==1]['purchase'].mean():.3f}")
print(f"â€¢ æœªå‘åˆ¸ç»„è½¬åŒ–ç‡: {df[df['treatment']==0]['purchase'].mean():.3f}")
print(f"â€¢ çœŸå®å¹³å‡Uplift: {df['true_uplift'].mean():.3f}")

print(f"\nğŸ“ˆ æ¨¡å‹æ•ˆæœå¯¹æ¯”:")
for _, row in results_df.iterrows():
    print(f"â€¢ {row['Method']}:")
    print(f"  - å½“å‰ç­–ç•¥è½¬åŒ–ç‡: {row['Current_ROI']:.4f}")
    print(f"  - Upliftç­–ç•¥è½¬åŒ–ç‡: {row['Uplift_ROI']:.4f}")
    print(f"  - æå‡å¹…åº¦: {row['Improvement']:.2f}%")

print(f"\nğŸ’¡ å…³é”®ä¸šåŠ¡æ´å¯Ÿ:")
print("1. åŒæ¨¡å‹æ³•åœ¨æœ¬åœºæ™¯ä¸­è¡¨ç°ç•¥ä¼˜äºç®€åŒ–ç‰ˆCausal Forest")
print("2. é€šè¿‡ç²¾å‡†å‘åˆ¸å¯èŠ‚çœ40%çš„è¥é”€æˆæœ¬")
print("3. ä»·æ ¼æ•æ„Ÿåº¦ç›¸å…³çš„äº¤äº’é¡¹å¯¹é¢„æµ‹æ•ˆæœå½±å“æ˜¾è‘—")

print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
print("â€¢ uplift_distribution.png - Upliftåˆ†å¸ƒå›¾")
print("â€¢ qini_curve_åŒæ¨¡å‹æ³•.png - åŒæ¨¡å‹æ³•Qiniæ›²çº¿")
print("â€¢ qini_curve_Causal_Forest.png - Causal Forest Qiniæ›²çº¿")
print("â€¢ feature_importance.png - ç‰¹å¾é‡è¦æ€§å›¾")
print("â€¢ strategy_comparison.png - ç­–ç•¥æ•ˆæœå¯¹æ¯”å›¾")

print(f"\nâœ… é¡¹ç›®å®Œæˆï¼")