{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc929719",
   "metadata": {},
   "source": [
    "# 💬 NLP高阶任务解析与技术反思\n",
    "---\n",
    "\n",
    "## 📘 一、Seq2Seq 模型及其应用\n",
    "\n",
    "### 1. 基本定义与应用场景\n",
    "Seq2Seq（Sequence-to-Sequence）模型是一种将一个序列映射为另一个序列的神经网络结构，主要用于：\n",
    "- 机器翻译（Machine Translation）\n",
    "- 文本摘要（Text Summarization）\n",
    "- 对话生成（Dialogue Generation）\n",
    "- 语音识别、代码生成等任务\n",
    "\n",
    "### 2. Encoder-Decoder 架构详解\n",
    "- **Encoder**：读取输入序列，输出一个上下文表示（如隐藏状态或 attention matrix）\n",
    "- **Decoder**：以Encoder输出为条件，逐步生成目标序列\n",
    "\n",
    "常见实现：\n",
    "- LSTM/GRU + LSTM/GRU（早期）\n",
    "- Transformer（目前主流）\n",
    "\n",
    "### 3. 技术关键点\n",
    "| 关键机制 | 作用 |\n",
    "|----------|------|\n",
    "| **Attention** | 解决信息瓶颈问题，让Decoder能访问Encoder所有隐藏状态 |\n",
    "| **Teacher Forcing** | 在训练中用真实输出作为Decoder输入，加速收敛 |\n",
    "| **Position Encoding** | Transformer中用于编码词位置信息 |\n",
    "| **Mask机制** | 防止模型在训练时看到未来信息（例如Causal Mask） |\n",
    "\n",
    "### 4. 小模型 vs 大模型（结构对比）\n",
    "\n",
    "| 模型类型 | 示例 | 特点 |\n",
    "|----------|------|------|\n",
    "| 小模型 | RNN、LSTM | 参数少，适合小数据集，但难以捕捉长依赖 |\n",
    "| 大模型 | Transformer、GPT | 并行计算强，能捕捉长距离依赖，训练数据需求大 |\n",
    "\n",
    "### 5. 解码策略：Greedy vs Beam Search\n",
    "| 策略 | 描述 | 优缺点 |\n",
    "|------|------|--------|\n",
    "| Greedy Search | 每一步选概率最大token | 快速但易陷入局部最优 |\n",
    "| Beam Search | 保留k个候选序列并延展 | 更优质量，但耗时更长 |\n",
    "\n",
    "---\n",
    "\n",
    "## 📕 二、文本分类（Text Classification）\n",
    "\n",
    "### 1. 应用场景\n",
    "- 情感分析（Sentiment Analysis）\n",
    "- 新闻分类、主题分类\n",
    "- 意图识别（Intent Detection）\n",
    "- 法律、金融文本分类\n",
    "\n",
    "### 2. 分类方法概述\n",
    "- 传统机器学习：TF-IDF + SVM / Logistic Regression / NB\n",
    "- 深度学习方法：\n",
    "  - CNN + Embedding\n",
    "  - RNN/LSTM\n",
    "  - Transformer（如BERT、DistilBERT）\n",
    "\n",
    "### 3. 评价指标（从混淆矩阵出发）\n",
    "\n",
    "| 指标 | 含义 | 适用场景 |\n",
    "|------|------|-----------|\n",
    "| Accuracy | 总体准确率 | 类别平衡任务 |\n",
    "| Precision | 准确预测正类的比例 | 误报代价高（如医疗） |\n",
    "| Recall | 召回所有正类的比例 | 漏报代价高（如安检） |\n",
    "| F1 Score | Precision 和 Recall 的调和平均 | 精确率与召回需兼顾 |\n",
    "| ROC-AUC | 模型对正负类的排序能力 | 非平衡数据更稳健 |\n",
    "| LogLoss | 概率预测的置信度误差 | 需要模型输出概率的任务 |\n",
    "\n",
    "### 4. 模型训练注意事项\n",
    "- 类别不平衡 → 用Focal Loss、加权损失\n",
    "- 数据预处理 → 去停用词、分词、词嵌入\n",
    "- 特征工程（在ML方法中） → N-gram、词频、情感词表等\n",
    "\n",
    "---\n",
    "\n",
    "## 📗 三、问答系统（Question Answering）\n",
    "\n",
    "### 1. 问答系统类型\n",
    "| 类型 | 描述 | 示例 |\n",
    "|------|------|------|\n",
    "| 封闭域QA | 给定段落中找答案 | SQuAD |\n",
    "| 开放域QA | 从大规模语料中找答案 | Google Search QA |\n",
    "| 多轮QA | 上下文对话式问答 | Chatbot |\n",
    "| 生成式QA | 模型自己组织答案 | GPT类问答 |\n",
    "| 检索增强生成（RAG） | 检索文档+生成回答 | ChatGPT-Retrieval Plugin |\n",
    "\n",
    "### 2. 构建流程\n",
    "\n",
    "1. **问题理解** → 语法分析、NER、句法依赖\n",
    "2. **文档检索（开放域）** → TF-IDF / BM25 / FAISS 向量检索\n",
    "3. **候选段落排序**\n",
    "4. **阅读理解或答案生成**\n",
    "   - 抽取式（如BERT for SQuAD）\n",
    "   - 生成式（如T5、GPT）\n",
    "5. **答案后处理** → 过滤、排序、置信度评分\n",
    "\n",
    "### 3. QA评估指标\n",
    "\n",
    "| 场景 | 指标 | 含义 |\n",
    "|------|------|------|\n",
    "| 抽取式QA | Exact Match, F1 | 预测答案和参考答案的重叠程度 |\n",
    "| 生成式QA | BLEU, ROUGE | 生成回答与参考答案的匹配度 |\n",
    "| 检索模块 | Recall@k, MRR | 是否在前k个段落中找到答案 |\n",
    "| 用户层面 | 响应时间、满意度 | 实用性指标 |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 小结：适配模型选型建议\n",
    "\n",
    "| 任务 | 小模型适用 | 大模型适用 |\n",
    "|------|------------|------------|\n",
    "| 机器翻译 | LSTM适合教育场景 | Transformer适合产业级翻译 |\n",
    "| 情感分析 | LogisticRegression 快速部署 | DistilBERT/ERNIE 表现更好 |\n",
    "| QA系统 | 抽取式BERT简单稳健 | RAG/GPT适合复杂开放问答 |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
